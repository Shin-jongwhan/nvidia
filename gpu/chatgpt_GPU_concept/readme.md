# GPU concept
### chatGPT에게 GPU가 작동하는 원리에 대해 물어보았다.
### https://chatgpt.com/share/6734553a-bfe0-800b-ab2c-5a8952f6832a
### <br/><br/>

## 질문 사항
### * 질문한 내용을 그대로 가져 왔고, 일부 오타가 있을 수 있음
- 비용적인 측면에서, AWS에서 AMI를 별도로 만들어서 하는 게 좋을까 아니면 같은 설치를 user data 등을 이용해서 하는 게 좋을까? 어떤게 더 비용을 최적화 할 수 있는지 알려줘
- DRAM과 SSD의 속도 비교는 어떻게 할 수 있어?
- SSD 대기 시간
- NVMe SSD기준으로 이야기해줘. 그런데 DRAM은 속도 단위가 MHz이고, SSD는 단위가 MB/s잖아. 어떻게 비교해?
- 왜 3200×64×2에서 왜 2배를 곱해줘?
- 폰 노이만 아키텍처에서 SSD와 같은 저장 공간도 그 아키텍처에 원래 포함이 되는 걸까?
- 3200×64×2 / 8 에서 8비트를 나눠주는 이유
- GPU에는 쿠다 코어가 있잖아? 그런데 이거는 물리적인 실제 코어가 아닌, 물리적 코어를 논리적인 코어로 나눈 것이라고 들었어. 맞을까?
- H100에 대해서 알아?
- H100의 쿠다 코어 수
- 일반적인 GPU와 H100이 뭐가 다를까
- 그럼 일반적인 하드웨어 구조는 동일한 걸까?
- NVLink 및 NVSwitch는 별도의 하드웨어야?
- 하드웨어 기술이라는 건 하드웨어를 말하는 거지?
- 궁금한 게 있어. GPU의 메모리도 RAM일까?
- 그러면, RAM에서 GPU memory로 데이터를 어떻게 전송해?
- Unified Memory에 대해 궁금한 점이 있어. 물리적으로 RAM과 GPU memory는 분리되어 있는데, 어떻게 하나의 메모리 공간을 유지하는 거야? 어디에?
- 그러면 RAM과 GPU memory 각각에 같은 데이터가 읽혀져 있는 상태이고, 같은 데이터에 대한 주소 공간을 소프트웨어가 별도로 저장하고 있다는 거지?
- 궁금한 게 있어. 동영상 스트리밍이나 게임 그래픽은 GPU에서 처리하잖아. 그 데이터는 어떻게 통신하고 연산되는 걸까?
- 데이터 형태는 주소 공간으로 조회할 수 있는 매트릭스와 같은 형태야?
- 내가 이해한 게 맞는지 체크해줘.
  1. 먼저 CPU에서 동영상 데이터를 decoding한 다음, 그 데이터를 GPU memory로 보내. 
  2. GPU 매트릭스와 같은 형태로 연산을 해.
- 그럼 영상의 데이터는 계속 GPU에 남아 있는 거야?
- CPU에서는 순차적으로 프레임 데이터를 저장하고, 그리고 순차적으로 GPU에 보내는 걸까?
- 그런데 말야. 예를 들어 유튜브 영상에서 10분 짜리 영상이 있어. 내가 7분 프레임에서 갑자기 3분 프레임으로 넘어갔어. 그러면 어떻게 돼?
- 여기서 궁금한 게 2가지가 있어.
  1. 3분 지점의 키 프레임을 찾는 작업은 시간복잡도가 어떻게 돼? 그 작동하는 방법. 
  2. 영상은 블록처럼, 예를 들어 몇 분의 프레임은 저장하고 다시 렌더링을 안 하고 바로 보여질 수 있게 하잖아. 이거는 어떻게 하는 거야?
- 캐싱 전략은 게임 그래픽에서도 동일한 원리로 작동하는 거지?
- 오케이.. 키 프레임에 대해 좀 더 궁금해. 예를 들어 100000 프레임이 있어. 근데 키 프레임으로 이 모든 프레임을 저장하는 거야?
- 그런데 머신 러닝에서 매트릭스 계산이랑 동영상 스트리밍에서 데이터 처리 방식이랑 똑같네?
- 좀 더 궁금한 게 있어. GPU는 코어 수가 많기 때문에 한 매트릭스(n x n)를 (코어 수 만큼) 동시에 연산이 가능하다는 거는 알겠어. 맞지?
- 여기서 궁금한 게 있어. 예를 들어서 100 x 100 매트릭스가 있어. 그리고 GPU 코어 수는 이 매트릭스를 한 번에 계산하는 데에 충분한 코어가 있다고 가정하자.<br/>
  그럼 100 x 100의 매트릭스는 동시에 연산이 수행한다고 하더라도 결국 순차적으로 접근해야 하는거 아니야?
- 2번 타일링에 대해 좀 더 자세히 설명해줘. 이 타일들은 **공유 메모리(shared memory)**에 로드된다 부분에서 궁금해. 그럼 각 코어에게도 메모리가 있고, GPU 메모리도 따로 있는 거야?
- 글로벌 메모리는 매트릭스 전체를 저장하고, 공유 메모리는 각 코어에 할당될 데이터들을 매핑하고, 레지스터는 변수를 저장해서 연산하는 작업을 하는 거지?
- 아까 이야기했던 캐시에 대해서 좀 더 알려줘.<br/>
  3. 캐시와 공유 메모리를 통한 병렬 접근<br/>
  GPU는 캐시와 공유 메모리를 활용하여 자주 사용하는 데이터는 메모리에서 가져오는 대신, 캐시에 저장하여 빠르게 접근할 수 있습니다.<br/>
  예를 들어, 매트릭스 곱셈에서 한 타일에 대한 연산을 여러 코어가 반복해서 사용해야 하는 경우, 공유 메모리에 로드된 데이터를 코어들이 재사용하여 메모리 접근 시간을 줄입니다.
- 어... 갑자기 궁금한 게... 글로벌 메모리에서 공유 메모리로 할당할 때의 시간복잡도는 어떻게 돼?
- 아하... 그치?? 내가 궁금한 게 그거였어.
- 질문이 있어. <br/>
  1. HBM, GDDR이라고 하는 건 글로벌 메모리야 아니면 공유 메모리야? <br/>
  2. 캐시는 GPU에 있는 메모리야?
- 아래는 무슨 뜻이야? GPU 메모리가 칩 외부에 위치해 있다?? 이해가 잘 안 된다.<br/>
  글로벌 메모리는 GPU 칩 외부에 위치하며, 대규모 데이터를 저장하고 처리할 수 있는 용량을 가지고 있지만, 접근 속도는 상대적으로 느립니다.
- 아하. 그러니까 GPU도 메인 보드와 같이 한 보드가 있고, 거기에 코어를 담당하는 칩 부분이 있고, 메모리를 담당하는 칩 부분이 있다는 거지?
- 궁금한 게 있어. CUDA 코어, Tensor Core의 차이에 대해 설명해줘.
- FP16과 FP32, FP64는 무슨 뜻이야
- 모르는 단어가 많이 있네. ㅎㅎ 혼합 정밀도(Mixed Precision) 연산, 행렬 곱셈과 누적(Matrix Multiply and Accumulate, MMA), Single Instruction, Multiple Data (SIMD)  구조에 대해 설명해줘.
- 내가 이해한 게 맞는지 체크해줘.<br/>
  1. 컴퓨터에서는 비트로 처리를 해. 그리고 그 비트는 기초 논리 회로로 처리를 하고. 비트가 많아질 수록 그만큼 시간복잡도는 늘어나는거야. 그래서 정밀도가 작을 수록 빠른 연산을 할 수 있어. 맞을까?<br/>
  2. MMA, SIMD 방식은 마지 numpy의 연산 방법을 연상하게 돼. 똑같은 거야?
- 오케이. 그럼 다시 돌아가서... CUDA 코어는 스트림 프로세서라고도 하잖아. tensor core도 이 부류일까?
- 아하.. 오케이. tensor core는 매트릭스 연산에 특화되어 있고, CUDA 코어는 개별 연산에 특화된 상태구나. 맞지?
- 그런데 CUDA core랑 tensor core가 같은 GPU에 있어?
- 일반적인 GPU는 어때? 예를 들어 RTX 4090 말야.
- tensor core 개수는 어떻게 돼?
- RT core에 대해서 설명해줘
- 음... 기능적인 건 알겠어. 그런데 궁금한 게 있어. GPU에 DLSS와 같은 기능이 있다는 건 별도의 ALU 설계가 들어가 있다는 거야?
- DLSS는 CNN과 같은 기법을 사용하는 걸까?
- 그런데, RT core랑 tensor core는 다른 거잖아. RT core와 tensor core의 관계에 대해 좀 더 알려줘. 그리고 학습 모델은 이미 엔비디아에서 수행되어 탑재된거야?
- 모델 업데이트 과정에 대해서도 알려줘서 고마워. 추론만 가능하다는 건, 신경망과 weight 등이 이미 어딘가 저장되어 있다는 거지?
- RT core 연산 방법에 대해 좀 더 자세히 알려줘
- 아 맞다, 그전에 궁금한 게 있는데 GPU 코어가 항상 2의 제곱수더라고?? 왜 그런거야?
- GPU 관련 자격증같은 것도 있을까?
